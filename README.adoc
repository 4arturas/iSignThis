= K8s
v1.0, 2020-11-14
:example-caption!:
:sectnums:
:sectnumlevels: 7
{set:sourcedir:k8s}

== What is K8s

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=139&end=321" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

== Main K8s Components

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=321&end=1350" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== Nodes and Pods
- Node - physical or virtual machine.
- Smalles unit of k8s is pod. Pod is abstraction over container. Usually 1 application per Pod. Pods in k8s commuticates via virtual k8s network. Echa Pod gets its own internal IP address.

=== Service and Ingress
- Service - permanent IP address that can be attached to to each Pod.
- Lifecycle of Pod and Service NOT connected, if Pod dies the service and its ip address will stay, so you don't have to change that end point any more.
- Ingress. Application needs to be acced from outside and for this you would have to create an external service. External service is the service that opens the communication from external sources. The request goes to Ingress and then it does the forwarding then to the service.

[plantuml]
....
partition Node1 #LightSkyBlue {
    - Ingress
    partition my-app(pod) {
        - Service
    }
    partition DB(pod) {
        -Service
    }
}
....

=== ConfigMap and Secret
- ConfigMap is external configuration of your application(url of a database or some other services, db username and password is not secure place to keep in ConfigMap). if we change endpoint of the servcice we just adjust the ConfigMap
- Secret is just like ConfigMap, but the difference is that it is used to store secret data, credentials for example. The information is in base64 encoded format

=== Volumes
If DB container gets restarted the data would be gone. Volumes attaches a physical storage on a hard drive to Pod. That storage can be either on a local machine, meaning on the same server node where the pod is running or it could be on a remote storage, meaning outside of the k8s cluster, it could be a cloud storage or your own premise storage which is not part of k8s cluster.

=== Deployment and Stateful Set

== K8s Architecture

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=1350&end=2088" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== Two type of nodes
- Master
- Slave
----
- Each node has multiple Pods on it.
- Three processes must be installed on every node
----
    * container runtime
    *
----
- Worker Nodes do the actual work

----


[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
Each node has multiple Pods on it
Three processes must be installed on every node
|_ container runtime
|_ kubelet(1)
|_ kube proxy(3)
Worker Nodes do the actual work
end legend
----

----
(1) Kubelete - interacts with both - the container and node, Kubelet starts the pod with a container inside, and then assigning resources from that node to the container like cpu, ram and storage resources.
----
----
(2) Servlet is a sort of loadbalancer that basically caches the request directed to the pod or the application and the forwads it to the repective pod
----
----
(3) Kube proxy forwards the requests
----

[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
How do you interact with this cluster
|_How to:
  |_ schedule pod?
  |_ monitor?
  |_ re-schedule/re-start pod?
  |_ join a new Node?
end legend
----
All these managing processes are done by *Master Nodes*
[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
Master processes
|_Api server(cluster gateway)
|_Scheduler
|_Controller manager
|_etcd(key value store of a cluster state. it is a cluster brain)
  |_ Is the cluster healthy?
  |_ What resources are available?
  |_ Did the cluster state change?
end legend
----


[plantuml]
----
title API Server
SomeRequest -> APIServer
APIServer -> ValidateRequest
ValidateRequest -> "other processes"
"other processes" -> Pod
----

[plantuml]
----
title Schedule
"Schedule New Pod" -> "API Server"
"API Server" -> Scheduler
Scheduler -> "Where to put the Pod?"
"Where to put the Pod?" -> Kubelet

----

[plantuml]
----
title Controller manager
"Controller manager" -> Scheduler
Scheduler -> Kubelet
Kubelet -> Pod

----

=== Example cluster set-up

== Minikube and kubectl - Local Setup

=== Master
==== Install
[source]
----
hostnamectl set-hostname master.art.local
echo "192.168.56.10 master.art.local master" >> /etc/hosts
echo "192.168.56.11 node1.art.local node1" >> /etc/hosts
echo "192.168.56.12 node2.art.local node2" >> /etc/hosts
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--node-ip=192.168.56.10 --flannel-iface=enp0s8" sh -
----
==== Debug
[source]
----
systemctl status k3s
journalctl -f --unit k3s
watch -n3 kubectl get nodes
----
==== Config file
[source]
----
cat /etc/rancher/k3s/k3s.yaml
chmod 644 /etc/rancher/k3s/k3s.yaml
----
==== Token
[source]
----
cat /var/lib/rancher/k3s/server/token
----
==== Uninstall
[source]
----
/usr/local/bin/k3s-uninstall.sh
----
=== Node
==== Install
[source]
----
hostnamectl set-hostname node1.art.local node1
echo "192.168.56.10 master.art.local master" >> /etc/hosts
echo "192.168.56.11 node1.art.local node1" >> /etc/hosts
echo "192.168.56.12 node2.art.local node2" >> /etc/hosts
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--node-ip=192.168.56.11 --flannel-iface=enp0s8" K3S_URL="https://192.168.56.10:6443" K3S_TOKEN="K108ada0b700e91cf2201586cedb9e0f26b7a5a7923fb551affb7aca393e03630c5::server:8568fd7b7e57846f03294ea4f6a3de00" sh -
----
==== Debug
[source]
----
journalctl -f --unit k3s-agent
----
==== Uninstall
[source]
----
/usr/local/bin/k3s-agent-uninstall.sh
----
//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=2088&end=2693" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

== Main Kubectl Commands - K8s CLI

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=2693&end=3724" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
    FontSize 17

}
skinparam classFontColor red
skinparam classFontSize 10
skinparam classFontName Aapex
legend
CRUD commands
|_Create deployment
  |_ kubectl create deployment [name]
|_Edit deployment
  |_ kubectl edit deployment [name]
|_Delete deployment
  |_kubectl delete deployment [name]
|_etcd(key value store of a cluster state. it is a cluster brain)
Status of different K8s components
|_ kubectl get
  |_ nodes
  |_ pod
  |_ services
  |_ replicaset
  |_ deployment
Debbuging pods
|_Log to console
  |_ kubectl logs [pod name]
|_ Get Interacitve terminal
  |_ kubectl exec -it [pod name] --bin/bash
end legend
----

[source]
----
kubectl create -h
kubectl create deployment nginx-depl --image=nginx
kubectl get deployment
kubectl get pod
kubectl get replicaset
----
[plantuml]
----
title Layers of abstration
Deployment -> ReplicaSet: manages
ReplicaSet -> Pod: manages all the replicas of that pod
Pod -> Container: is abstration of the
----
[source]
----
kubectl edit deployment nginx-depl
- change image to nginx:1.16
kubectl get pod
kubectl get replicaset
----

=== Debugging pods
[source]
----
kubectl get pods
kubectl logs nginx-depl-7fc44fc5d4-pc9wr
kubectl create deployment mongo-depl --image=mongo
kubectl get pods
kubectl logs mongo-depl-5fd6b7d4b4-lr6dv
kubectl describe pod mongo-depl-5fd6b7d4b4-lr6dv
----
[source]
----
kubectl get pods
kubectl exec -it mongo-depl-5fd6b7d4b4-lr6dv -- bin/bash
----
=== Delete deployment
[source]
----
kubectl get deployment
kubectl delete deployment mongo-depl
kubectl get pod
kubectl get replicaset
----
=== Apply configuration file

[source]
.k8s/nginx-deployment.yaml
----
include::k8s/nginx-deployment.yaml[]
----
[source]
----
kubectl apply -f nginx-deployment.yaml
kubectl get deployment
kubectl get pod
kubectl describe service nginx-service
kubectl get pods -o wide
----

[source]
.k8s/nginx-service.yaml
----
include::k8s/nginx-service.yaml[]
----
[source]
----
kubectl apply -f nginx-service.yaml
kubectl get service
----

=== Delete components using configuration files
[source]
----
kubectl delete -f nginx-deployment.yaml
kubectl delete -f nginx-service.yaml

----

== K8s YAML Configuration File
//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=3724&end=4577" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++
=== The 3 parts of configuration file
Each K8s configuration file has 3 parts

- Metatada
- Specification
- Status - is not in the file it will be automatically generated and added by K8s. In file is "Desired status", K8s generates "Actual status". If Desired and Actual does not match then K8s knows that somthing to be fixed there, so it is going to fix it. It is the self-healing feature K8s provides. This information comes from etcd.

[source]
.k8s/deployment-structure.yaml
----
include::k8s/deployment-structure.yaml[]
----
[source]
.k8s/service-structure.yaml
----
include::k8s/service-structure.yaml[]
----

status is get using
[source]
----
kubectl get deployment nginx-deployment -o yaml
kubectl get deployment nginx-deployment -o yaml > nginx-deployment-result.txt
----

=== Format of configuration file
YAML


== Complete Application Setup with Kubernetes Compoenents

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=4577&end=6377" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== Demo Project: MongoDB and MongoExpress

==== mongoDB
- Deployment / Pod
- Service
- ConfigMap
- Secret

==== mongo-express
===== ConfigMap
- mongoDB url

===== Secret
- DB user
- DB password

===== Accessible through browser
- in order to do that we will create external service

[plantuml]
.....
"Browser" -> "Mongo Express External Service"
"Mongo Express External Service" -> "Mongo Express"
"Mongo Express" -> "Mongo DB Internal Service"
"Mongo DB Internal Service" -> "MongoDB Pod": "Secret: DB usr, DB pwd
.....

=== Lets create
==== Mongo DB Internal service
link:https://hub.docker.com/_/mongo[Mongo on docker hub]

===== Create a Secret

.k8s/mongo-db-secret.yaml
----
include::k8s/mongo-db-secret.yaml[]
----

[source]
----
echo -n 'username' | base64
echo -n 'password' | base64
----
[source]
----
kubectl apply -f mongo-db-secret.yaml
kubectl get secret
kubectl describe secret mongodb-secret
----

===== Create Deployment
.k8s/mongo-db-deployment.yaml
----
include::k8s/mongo-db-deployment.yaml[]
----

[source]
----
kubectl apply -f mongo-db-deployment.yaml
kubectl get deployment
kubectl get pod --watch
----

===== Create Service

.k8s/mongo-db-service.yaml
----
include::k8s/mongo-db-service.yaml[]
----
[source]
----
kubectl apply -f mongo-db-service.yaml
kubectl get service
kubectl get pod -o wide
kubectl get all | grep mongodb
----

https://youtu.be/X48VuDVv0do?t=5604

=== Mongo Express
==== Mongo Express ConfigMap
.k8s/mongo-express-configmap.yaml
----
include::k8s/mongo-express-configmap.yaml[]
----
[source]
----
kubectl apply -f mongo-express-configmap.yaml
kubectl describe configmap mongodb-configmap
----
==== Mongo Express Deployment
link:https://hub.docker.com/_/mongo-express[Mongo-express on docker hub]

.k8s/mongo-express-deployment.yaml
----
include::k8s/mongo-express-deployment.yaml[]
----
[source]
----
kubectl apply -f mongo-express-deployment.yaml
----

==== Mongo Express External Service

.k8s/mongo-express-service.yaml
----
include::k8s/mongo-express-service.yaml[]
----
[source]
----
kubectl apply -f mongo-express-service.yaml
kubectl get service
----

http://192.168.56.10:30000/

== Organizing your components with K8s Namespaces

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=4577&end=6377" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

== K8s Ingress explained
== Helm - Package Manager
== Persisting Data in K8s with Volumes
== Deploying Stateful Apps with StatefulSet
== K8s Services explained
