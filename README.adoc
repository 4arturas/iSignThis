= K8s
v1.0, 2020-11-14
:example-caption!:
:sectnums:
:sectnumlevels: 7
{set:sourcedir:k8s}

== What is K8s

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=139&end=321" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

== Main K8s Components

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=321&end=1350" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== Nodes and Pods
- Node - physical or virtual machine.
- Smalles unit of k8s is pod. Pod is abstraction over container. Usually 1 application per Pod. Pods in k8s commuticates via virtual k8s network. Echa Pod gets its own internal IP address.

=== Service and Ingress
- Service - permanent IP address that can be attached to to each Pod.
- Lifecycle of Pod and Service NOT connected, if Pod dies the service and its ip address will stay, so you don't have to change that end point any more.
- Ingress. Application needs to be acced from outside and for this you would have to create an external service. External service is the service that opens the communication from external sources. The request goes to Ingress and then it does the forwarding then to the service.

[plantuml]
....
partition Node1 #LightSkyBlue {
    - Ingress
    partition my-app(pod) {
        - Service
    }
    partition DB(pod) {
        -Service
    }
}
....

=== ConfigMap and Secret
- ConfigMap is external configuration of your application(url of a database or some other services, db username and password is not secure place to keep in ConfigMap). if we change endpoint of the servcice we just adjust the ConfigMap
- Secret is just like ConfigMap, but the difference is that it is used to store secret data, credentials for example. The information is in base64 encoded format

=== Volumes
If DB container gets restarted the data would be gone. Volumes attaches a physical storage on a hard drive to Pod. That storage can be either on a local machine, meaning on the same server node where the pod is running or it could be on a remote storage, meaning outside of the k8s cluster, it could be a cloud storage or your own premise storage which is not part of k8s cluster.

=== Deployment and Stateful Set

== K8s Architecture

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=1350&end=2088" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== Two type of nodes
- Master
- Slave
----
- Each node has multiple Pods on it.
- Three processes must be installed on every node
----
    * container runtime
    *
----
- Worker Nodes do the actual work

----


[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
Each node has multiple Pods on it
Three processes must be installed on every node
|_ container runtime
|_ kubelet(1)
|_ kube proxy(3)
Worker Nodes do the actual work
end legend
----

----
(1) Kubelete - interacts with both - the container and node, Kubelet starts the pod with a container inside, and then assigning resources from that node to the container like cpu, ram and storage resources.
----
----
(2) Servlet is a sort of loadbalancer that basically caches the request directed to the pod or the application and the forwads it to the repective pod
----
----
(3) Kube proxy forwards the requests
----

[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
How do you interact with this cluster
|_How to:
  |_ schedule pod?
  |_ monitor?
  |_ re-schedule/re-start pod?
  |_ join a new Node?
end legend
----
All these managing processes are done by *Master Nodes*
[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
}
legend
Master processes
|_Api server(cluster gateway)
|_Scheduler
|_Controller manager
|_etcd(key value store of a cluster state. it is a cluster brain)
  |_ Is the cluster healthy?
  |_ What resources are available?
  |_ Did the cluster state change?
end legend
----


[plantuml]
----
title API Server
SomeRequest -> APIServer
APIServer -> ValidateRequest
ValidateRequest -> "other processes"
"other processes" -> Pod
----

[plantuml]
----
title Schedule
"Schedule New Pod" -> "API Server"
"API Server" -> Scheduler
Scheduler -> "Where to put the Pod?"
"Where to put the Pod?" -> Kubelet

----

[plantuml]
----
title Controller manager
"Controller manager" -> Scheduler
Scheduler -> Kubelet
Kubelet -> Pod

----

=== Example cluster set-up

== Minikube and kubectl - Local Setup

=== Master
==== Install
[source]
----
hostnamectl set-hostname master.art.local
echo "192.168.56.10 master.art.local master" >> /etc/hosts
echo "192.168.56.11 node1.art.local node1" >> /etc/hosts
echo "192.168.56.12 node2.art.local node2" >> /etc/hosts
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--node-ip=192.168.56.10 --flannel-iface=enp0s8" sh -
----
==== Debug
[source]
----
systemctl status k3s
journalctl -f --unit k3s
watch -n3 kubectl get nodes
----
==== Config file
[source]
----
cat /etc/rancher/k3s/k3s.yaml
chmod 644 /etc/rancher/k3s/k3s.yaml
----
==== Token
[source]
----
cat /var/lib/rancher/k3s/server/token
----
==== Uninstall
[source]
----
/usr/local/bin/k3s-uninstall.sh
----
=== Node
==== Install
[source]
----
hostnamectl set-hostname node1.art.local node1
echo "192.168.56.10 master.art.local master" >> /etc/hosts
echo "192.168.56.11 node1.art.local node1" >> /etc/hosts
echo "192.168.56.12 node2.art.local node2" >> /etc/hosts
curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--node-ip=192.168.56.11 --flannel-iface=enp0s8" K3S_URL="https://192.168.56.10:6443" K3S_TOKEN="K108ada0b700e91cf2201586cedb9e0f26b7a5a7923fb551affb7aca393e03630c5::server:8568fd7b7e57846f03294ea4f6a3de00" sh -
----
==== Install specifi K3s version
link:https://github.com/rancher/k3s/releases[K3s versions]
[source]
----
curl -sfL https://get.k3s.io | INSTALL_K3S_VERSION=v1.18.12+k3s1 sh -
----
==== Debug
[source]
----
journalctl -f --unit k3s-agent
----
==== Uninstall
[source]
----
/usr/local/bin/k3s-agent-uninstall.sh
----
//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=2088&end=2693" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

== Main Kubectl Commands - K8s CLI

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=2693&end=3724" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

[plantuml]
----
skinparam Legend {
	BackgroundColor transparent
	BorderColor transparent
    FontSize 17

}
skinparam classFontColor red
skinparam classFontSize 10
skinparam classFontName Aapex
legend
CRUD commands
|_Create deployment
  |_ kubectl create deployment [name]
|_Edit deployment
  |_ kubectl edit deployment [name]
|_Delete deployment
  |_kubectl delete deployment [name]
|_etcd(key value store of a cluster state. it is a cluster brain)
Status of different K8s components
|_ kubectl get
  |_ nodes
  |_ pod
  |_ services
  |_ replicaset
  |_ deployment
Debbuging pods
|_Log to console
  |_ kubectl logs [pod name]
|_ Get Interacitve terminal
  |_ kubectl exec -it [pod name] --bin/bash
end legend
----

[source]
----
kubectl create -h
kubectl create deployment nginx-depl --image=nginx
kubectl get deployment
kubectl get pod
kubectl get replicaset
----
[plantuml]
----
title Layers of abstration
Deployment -> ReplicaSet: manages
ReplicaSet -> Pod: manages all the replicas of that pod
Pod -> Container: is abstration of the
----
[source]
----
kubectl edit deployment nginx-depl
- change image to nginx:1.16
kubectl get pod
kubectl get replicaset
----

=== Debugging pods
[source]
----
kubectl get pods
kubectl logs nginx-depl-7fc44fc5d4-pc9wr
kubectl create deployment mongo-depl --image=mongo
kubectl get pods
kubectl logs mongo-depl-5fd6b7d4b4-lr6dv
kubectl describe pod mongo-depl-5fd6b7d4b4-lr6dv
----
[source]
----
kubectl get pods
kubectl exec -it mongo-depl-5fd6b7d4b4-lr6dv -- bin/bash
----
=== Delete deployment
[source]
----
kubectl get deployment
kubectl delete deployment mongo-depl
kubectl get pod
kubectl get replicaset
----
=== Apply configuration file

.k8s/nginx-deployment.yaml
----
include::k8s/nginx-deployment.yaml[]
----
[source]
----
kubectl apply -f nginx-deployment.yaml
kubectl get deployment
kubectl get pod
kubectl describe service nginx-service
kubectl get pods -o wide
----

[source]
.k8s/nginx-service.yaml
----
include::k8s/nginx-service.yaml[]
----
[source]
----
kubectl apply -f nginx-service.yaml
kubectl get service
----

=== Delete components using configuration files
[source]
----
kubectl delete -f nginx-deployment.yaml
kubectl delete -f nginx-service.yaml

----

== K8s YAML Configuration File
//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=3724&end=4577" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++
=== The 3 parts of configuration file
Each K8s configuration file has 3 parts

- Metatada
- Specification
- Status - is not in the file it will be automatically generated and added by K8s. In file is "Desired status", K8s generates "Actual status". If Desired and Actual does not match then K8s knows that somthing to be fixed there, so it is going to fix it. It is the self-healing feature K8s provides. This information comes from etcd.

[source]
.k8s/deployment-structure.yaml
----
include::k8s/deployment-structure.yaml[]
----
[source]
.k8s/service-structure.yaml
----
include::k8s/service-structure.yaml[]
----

status is get using
[source]
----
kubectl get deployment nginx-deployment -o yaml
kubectl get deployment nginx-deployment -o yaml > nginx-deployment-result.txt
----

=== Format of configuration file
YAML


== Complete Application Setup with Kubernetes Compoenents

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=4577&end=6377" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== Demo Project: MongoDB and MongoExpress

==== mongoDB
- Deployment / Pod
- Service
- ConfigMap
- Secret

==== mongo-express
===== ConfigMap
- mongoDB url

===== Secret
- DB user
- DB password

===== Accessible through browser
- in order to do that we will create external service

[plantuml]
.....
"Browser" -> "Mongo Express External Service"
"Mongo Express External Service" -> "Mongo Express"
"Mongo Express" -> "Mongo DB Internal Service"
"Mongo DB Internal Service" -> "MongoDB Pod": "Secret: DB usr, DB pwd
.....

=== Lets create
==== Mongo DB Internal service
link:https://hub.docker.com/_/mongo[Mongo on docker hub]

===== Create a Secret

.k8s/mongo-db-secret.yaml
----
include::k8s/mongo-db-secret.yaml[]
----

[source]
----
echo -n 'username' | base64
echo -n 'password' | base64
----
[source]
----
kubectl apply -f mongo-db-secret.yaml
kubectl get secret
kubectl describe secret mongodb-secret
----

===== Create Deployment
.k8s/mongo-db-deployment.yaml
----
include::k8s/mongo-db-deployment.yaml[]
----

[source]
----
kubectl apply -f mongo-db-deployment.yaml
kubectl get deployment
kubectl get pod --watch
----

===== Create Service

.k8s/mongo-db-service.yaml
----
include::k8s/mongo-db-service.yaml[]
----
[source]
----
kubectl apply -f mongo-db-service.yaml
kubectl get service
kubectl get pod -o wide
kubectl get all | grep mongodb
----

https://youtu.be/X48VuDVv0do?t=5604

=== Mongo Express
==== Mongo Express ConfigMap
.k8s/mongo-express-configmap.yaml
----
include::k8s/mongo-express-configmap.yaml[]
----
[source]
----
kubectl apply -f mongo-express-configmap.yaml
kubectl describe configmap mongodb-configmap
----
==== Mongo Express Deployment
link:https://hub.docker.com/_/mongo-express[Mongo-express on docker hub]

.k8s/mongo-express-deployment.yaml
----
include::k8s/mongo-express-deployment.yaml[]
----
[source]
----
kubectl apply -f mongo-express-deployment.yaml
----

==== Mongo Express External Service

.k8s/mongo-express-service.yaml
----
include::k8s/mongo-express-service.yaml[]
----
[source]
----
kubectl apply -f mongo-express-service.yaml
kubectl get service
----

http://192.168.56.10:30000/

== Organizing your components with K8s Namespaces

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=6377&end=7313" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

*Namespace* cluster inside a cluster

* K8s gives 4 namespaces out of the box
** *kube-system* is not meant for your own use
** *kube-public* - it has ConfigMap
** *kube-node-lease* - it holds information about heartbeats of nodes
** *default* - creates resources at the begining
[sorce]
----
kubectl get namespaces
kubectl create namespace my-namespace
kubectl get namespace
----
[plantuml]
....
partition "Kubernetes Cluster" #LightSkyBlue {
    partition kube-system {
    }
    partition kube-public {
    }
    partition default {
    }
}
....


What is the need for *namespaces*?::
* Logically group resources inside cluster.
[plantuml]
....
partition "Kubernetes Cluster" #LightSkyBlue {
    partition "Database\nnamespace" {
    }
    partition "Monitoring\nnamespace" {
    }
    partition "Elastic Stack\nnamespace" {
    }
    partition "Nginx-Ingress\nnamespace" {
    }
}
....
* Conflicts: Many teams, same application
* Rsource Sharing: Staging and Development
* Rsource Sharing: Blue/Green Deployment

[plantuml]
....
partition "Kubernetes Cluster" #LightSkyBlue {
    partition "Production Blue(active)" {
    }
    partition "Production Green(future)" {
    }
}
....
* Access and Resource Limits on Namespaces - for example we have two teams with different namespaces with separate resources on namespaces

Use cases when to use Namespaces::
. *Structure* components
. *Avoid conflicts* between teams
. *Share services* between different evironments
. *Access and Resource Limits* on Namespaces Level

Characteristics of Namespaces?::
. You can't access most resources from another Namespace
. Components, which can't be created within a Namespace: volume, node

Create components in Namespaces?

* First way
[source]
----
kubectl apply -f mysql-configmap.yaml --namespace=my-namespace
----
* Second in configuration file
[source]
----
metada.namespace: my-namepace
----

Change active namespace
[source]
----
Tool kubens
----

== K8s Ingress explained

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=7313&end=8658" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

=== What is Ingress?

==== External service vs. Ingress
- External service good for test cases

[plantuml]
....

partition "External Service" {
    partition "Browser" #DarkSalmon {
        start
        - 192.168.56.10:35010
    }

    partition "Kubernetes Cluster" #RoyalBlue {
        partition "my-app service" {
            - external
        }
        partition "my-app pod" {
            -internal
            stop
        }

    }
}
....

- Ingress IP address and ports is not opened!

[plantuml]
....
partition "Ingress" {
    partition "Browser" #DarkSalmon {
        (*) --> "https://my-app-2.com" <<Stage>>
    }
    partition "Kubernetes Cluster" #RoyalBlue {

        partition "my-app-2 ingress" #White {
            "https://my-app-2.com" -down-> "ingress" <<Stage>>
        }
        partition "my-app-2 service" #White {
            "ingress" -down-> "service" <<Stage>>
        }
        partition "my-app-2 pod" #White {
            "service" -down-> "pod" <<Bash>>
        }
    }
}
....

=== Ingress YAML configuration
- Internal service

.k8s/myapp-external-service.yaml
----
include::k8s/myapp-external-service.yaml[]
----
- Ingress

.k8s/myapp-ingress.yaml
----
include::k8s/myapp-ingress.yaml[]
----

=== How to configure Ingress?
Ingress YAML file is not enough for Ingress routing rules to work, we need in addition is an implementation for Ingress!
Implementation of Ingress is called Ingress Controller. Ingress controller is another Pod or set of Pods which evaluates, processes, manages redirections Ingress rules.
Ingress controller is the entry point in the cluster for all the requests to that domain subdomain rules.

There are many third-party implementations.
link:https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/[Ingress Controllers],
link:https://kubernetes.github.io/ingress-nginx/deploy/baremetal/[Bare metal]

==== Configure Ingress
TODO
[source]
----
GITHUB_URL=https://github.com/kubernetes/dashboard/releases

VERSION_KUBE_DASHBOARD=$(curl -w '%{url_effective}' -I -L -s -S ${GITHUB_URL}/latest -o /dev/null | sed -e 's|.*/||')

k3s kubectl create -f https://raw.githubusercontent.com/kubernetes/dashboard/${VERSION_KUBE_DASHBOARD}/aio/deploy/recommended.yaml
----
[source]
----
cat <<EOF > dashboard.admin-user.yml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: admin-user
  namespace: kubernetes-dashboard
EOF
----
[source]
----
cat <<EOF > dashboard.admin-user-role.yml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
name: admin-user
roleRef:
apiGroup: rbac.authorization.k8s.io
kind: ClusterRole
name: cluster-admin
subjects:
- kind: ServiceAccount
name: admin-user
namespace: kubernetes-dashboard
EOF
----
[soruce]
----
k3s kubectl create -f dashboard.admin-user.yml -f dashboard.admin-user-role.yml
----

.k8s/dashboard-ingress.yaml
----
include::k8s/dashboard-ingress.yaml[]
----
TODO
[source]
----
kubectl apply -f dashboard-ingress.yaml
# wait for ADDRESS to be assigned by K8s
watch kubectl get ingress -n kubernetes-dashboard
echo "10.0.2.15 dashboard.com" >> /etc/hosts
ping dashboard.com
curl dashboard.com
kubectl describe ingress dashboard-ingress -n kubernetes-dashboard
kubectl get ingress dashboard-ingress -n kubernetes-dashboard
----

[source]
----
kubectl -n kubernetes-dashboard edit svc kubernetes-dashboard
# change spec->type->ClusterIP to spec->type->NodePort
# add spec->ports->nodePort=32323
k3s kubectl -n kubernetes-dashboard describe secret admin-user-token | grep ^token
https://192.168.56.10:32323/
----

== Helm - Package Manager

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=8658&end=9488" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

package manager for K8s to package yaml files, like yum, apt

TODO

== Persisting Data in K8s with Volumes

//++++
//<iframe width="560" height="315" src="https://www.youtube.com/embed/X48VuDVv0do?start=9488&end=" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
//++++

== Deploying Stateful Apps with StatefulSet
== K8s Services explained



[plantuml]
....
  partition "My Partition" {
    (*) --> "Step1" <<Stage>>
    --> "Step2" <<Stage>>
    --> (*)
    }

    partition "First left partition" {
    "Step1" -left-> "First left Action" <<Bash>>
    }

    partition "First right partition" {
    "Step1" -right-> "First right Action" <<Bash>>
    }

    partition "Second left partition" {
    "Step2" -left-> "Second left Action" <<Bash>>
    }

    partition "Second right partition" {
    "Step2" -right-> "Second right Action" <<Bash>>
    }

    "Second left Action" .[hidden]u.> "First left Action"
....

[plantuml]
....
rectangle Arrows
rectangle Up
rectangle Down
rectangle Left
rectangle Right

Arrows -u-> Up
Arrows -d-> Down
Arrows -l-> Left
Arrows -r-> Right
....